# -*- coding: utf-8 -*-
"""Finetuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16FKDot-NXVNXFjaY3JUCV6sIYb-98qhx

# **Inicialización**
"""

!pip install datasets
!pip install transformers==4.3.3

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import pandas as pd
import numpy as np

import gc
import requests
import os

import transformers
from transformers import MBartForConditionalGeneration, MBartTokenizer
from transformers import BartForConditionalGeneration, BartTokenizer
from transformers import BertForMaskedLM, BertTokenizer
from transformers import RobertaForMaskedLM, RobertaTokenizer

from transformers import pipeline


from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, recall_score, precision_score


import torch
print("Cuda available" if torch.cuda.is_available() is True else "CPU")
print("PyTorch version: ", torch.__version__)
print(torch.cuda.is_available())
print(torch.cuda.current_device())
print(torch.cuda.device(0))
print(torch.cuda.device_count)
print(torch.cuda.get_device_name(0))

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount("/content/drive")
# %cd "/content/drive/MyDrive/TFG/Finetunning"

"""# **Primeras pruebas de Mask-Filling**"""

#The definition of bicycle, bike, cycle, wheel is: a wheeled vehicle that has two wheels and is moved by foot pedals.
nlpBartBase = pipeline("fill-mask", model="facebook/bart-base")
nlpRobertaBase = pipeline("fill-mask", model="roberta-base")

print("Bart:", nlpBartBase(f"The definition of {nlpBartBase.tokenizer.mask_token} is: a wheeled vehicle that has two wheels and is moved by foot pedals."))
print("Roberta:", nlpRobertaBase(f"The definition of {nlpRobertaBase.tokenizer.mask_token} is: a wheeled vehicle that has two wheels and is moved by foot pedals."))

"""# **Probando los modelos BART y RoBERTa con pequeños fine-tuning**"""

# 20 EPOCHS. 100 WORDS
!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="facebook/bart-base" \
    --train_file=./cleaned_data/wordnet.en/dictionary100words.txt \
    --do_train \
    --output_dir=./finetuned_models/bart-base/100_words \
    --per_device_train_batch_size=10 \
    --gradient_accumulation_steps=1 \
    --learning_rate=5e-5 \
    --num_train_epochs=20 \
    --save_total_limit=10 \
    --save_steps=500 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16

!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="roberta-base" \
    --train_file=./cleaned_data/wordnet.en/dictionary100words.txt \
    --do_train \
    --output_dir=finetuned_models/roberta-base/100_words \
    --per_device_train_batch_size=10 \
    --gradient_accumulation_steps=1 \
    --learning_rate=5e-5 \
    --num_train_epochs=20 \
    --save_total_limit=10 \
    --save_steps=500 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16

# 10 EPOCHS, 100 WORDS. BART AND ROBERTA-BASE
!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="facebook/bart-base" \
    --train_file=./cleaned_data/wordnet.en/dictionary100words.txt \
    --do_train \
    --output_dir=./finetuned_models/roberta-base/100_words \
    --per_device_train_batch_size=10 \
    --gradient_accumulation_steps=1 \
    --learning_rate=5e-5 \
    --num_train_epochs=10 \
    --save_total_limit=10 \
    --save_steps=500 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16
!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="roberta-base" \
    --train_file=./cleaned_data/wordnet.en/dictionary100words.txt \
    --do_train \
    --output_dir=finetuned_models/bart-base/100_words \
    --per_device_train_batch_size=10 \
    --gradient_accumulation_steps=1 \
    --learning_rate=5e-5 \
    --num_train_epochs=10 \
    --save_total_limit=10 \
    --save_steps=500 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16

#10 EPOCHS. 1000 WORDS. BART AND ROBERTA BASE
!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="facebook/bart-base" \
    --train_file=./cleaned_data/wordnet.en/dictionary1000words.txt \
    --do_train \
    --output_dir=finetuned_models/roberta-base/1000_words \
    --per_device_train_batch_size=10 \
    --gradient_accumulation_steps=1 \
    --learning_rate=5e-5 \
    --num_train_epochs=10 \
    --save_total_limit=10 \
    --save_steps=500 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16

!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="roberta-base" \
    --train_file=./cleaned_data/wordnet.en/dictionary1000words.txt \
    --do_train \
    --output_dir=finetuned_models/bart-base/1000_words \
    --per_device_train_batch_size=10 \
    --gradient_accumulation_steps=1 \
    --learning_rate=5e-5 \
    --num_train_epochs=10 \
    --save_total_limit=10 \
    --save_steps=500 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16

#5 EPOCHS. 10.000 WORDS, BART AND ROBERTA BASE
!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="facebook/bart-base" \
    --train_file=./cleaned_data/wordnet.en/dictionary10000words.txt \
    --do_train \
    --output_dir=finetuned_models/roberta-base/10000_words \
    --per_device_train_batch_size=10 \
    --gradient_accumulation_steps=1 \
    --learning_rate=5e-5 \
    --num_train_epochs=5 \
    --save_total_limit=10 \
    --save_steps=500 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16

!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="roberta-base" \
    --train_file=./cleaned_data/wordnet.en/dictionary10000words.txt \
    --do_train \
    --output_dir=finetuned_models/bart-base/10000_words \
    --per_device_train_batch_size=10 \
    --gradient_accumulation_steps=1 \
    --learning_rate=5e-5 \
    --num_train_epochs=5 \
    --save_total_limit=10 \
    --save_steps=500 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16

nlpBartBase = pipeline("fill-mask", model="facebook/bart-base")
nlpRobertaBase = pipeline("fill-mask", model="roberta-base")

nlpBart100Words = pipeline("fill-mask", model="/content/drive/MyDrive/TFG/Finetunning/finetuned_models/bart-base/100_words")
nlpRoberta100Words = pipeline("fill-mask", model="/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-base/100_words")

nlpBart1000Words = pipeline("fill-mask", model="/content/drive/MyDrive/TFG/Finetunning/finetuned_models/bart-base/1000_words")
nlpRoberta1000Words = pipeline("fill-mask", model="/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-base/1000_words")

nlpBart10000Words = pipeline("fill-mask", model="/content/drive/MyDrive/TFG/Finetunning/finetuned_models/bart-base/1000_words")
nlpRoberta10000Words = pipeline("fill-mask", model="/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-base/10000_words")

# Definicion no aprendida por ningun modelo: 
# The definition of pimpernel is: any of several plants of the genus Anagallis.
print("Bart Base:", nlpBartBase(f"The definition of {nlpBartBase.tokenizer.mask_token} is: any of several plants of the genus Anagallis."))
print("Roberta Base:", nlpRobertaBase(f"The definition of {nlpRobertaBase.tokenizer.mask_token} is: any of several plants of the genus Anagallis."))

print("Bart 1000 words:", nlpBart1000Words(f"The definition of {nlpBart1000Words.tokenizer.mask_token} is: any of several plants of the genus Anagallis."))
print("Roberta 1000 words:", nlpRoberta1000Words(f"The definition of {nlpRoberta1000Words.tokenizer.mask_token} is: any of several plants of the genus Anagallis."))

print("Bart 10000 words:", nlpBart10000Words(f"The definition of {nlpBart10000Words.tokenizer.mask_token} is: any of several plants of the genus Anagallis."))
print("Roberta 10000 words:", nlpRoberta10000Words(f"The definition of {nlpBartBase.tokenizer.mask_token} is: any of several plants of the genus Anagallis."))
print()

# Definicion aprendida por modelo de 10.000 palabras:
# The definition of bib is: top part of an apron; covering the chest.
print("Bart Base:", nlpBartBase(f"The definition of {nlpBartBase.tokenizer.mask_token} is: top part of an apron; covering the chest."))
print("Roberta Base:", nlpRobertaBase(f"The definition of {nlpRobertaBase.tokenizer.mask_token} is: top part of an apron; covering the chest."))

print("Bart 1000 words:", nlpBart1000Words(f"The definition of {nlpBart1000Words.tokenizer.mask_token} is: top part of an apron; covering the chest."))
print("Roberta 1000 words:", nlpRoberta1000Words(f"The definition of {nlpRoberta1000Words.tokenizer.mask_token} is: top part of an apron; covering the chest."))

print("Bart 10000 words:", nlpBart10000Words(f"The definition of {nlpBart10000Words.tokenizer.mask_token} is: top part of an apron; covering the chest."))
print("Roberta 10000 words:", nlpRoberta10000Words(f"The definition of {nlpRoberta10000Words.tokenizer.mask_token} is: top part of an apron; covering the chest."))
print()

# Definicion aprendida por los modelos de 1.000 y 10.000 palabras:
# The definition of ridicule is: language or behavior intended to mock or humiliate.
print("Bart Base:", nlpBartBase(f"The definition of {nlpBartBase.tokenizer.mask_token} is: language or behavior intended to mock or humiliate."))
print("Roberta Base:", nlpRobertaBase(f"The definition of {nlpRobertaBase.tokenizer.mask_token} is: language or behavior intended to mock or humiliate."))

print("Bart 1000 words:", nlpBart1000Words(f"The definition of {nlpBart1000Words.tokenizer.mask_token} is: language or behavior intended to mock or humiliate."))
print("Roberta 1000 words:", nlpRoberta1000Words(f"The definition of {nlpRoberta1000Words.tokenizer.mask_token} is: language or behavior intended to mock or humiliate."))

print("Bart 10000 words:", nlpBart10000Words(f"The definition of {nlpBart10000Words.tokenizer.mask_token} is: language or behavior intended to mock or humiliate."))
print("Roberta 10000 words:", nlpRoberta10000Words(f"The definition of {nlpRoberta10000Words.tokenizer.mask_token} is: language or behavior intended to mock or humiliate."))
print()

#The definition of bicycle, bike, cycle, wheel is: a wheeled vehicle that has two wheels and is moved by foot pedals.

print("Bart 10000 words:", nlpBart10000Words(f"The definition of {nlpBart10000Words.tokenizer.mask_token} is: a wheeled vehicle that has two wheels and is moved by foot pedals."))
print("Roberta 10000 words:", nlpRoberta10000Words(f"The definition of {nlpRoberta10000Words.tokenizer.mask_token} is: a wheeled vehicle that has two wheels and is moved by foot pedals."))
print()


#100 words. 20 epochs
# The definition of stricture is: severe criticism.
print("Bart 100 words:", nlpBart100Words(f"The definition of {nlpBart100Words.tokenizer.mask_token} is: severe criticism."))
print("Roberta 100 words:", nlpRoberta100Words(f"The definition of {nlpRoberta100Words.tokenizer.mask_token} is: severe criticism."))

"""# **Fine-tuneando con fichero de 100 definiciones para elegir mejores hiperparámetros**"""

!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="roberta-large" \
    --train_file=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words.txt \
    --do_train \
    --output_dir=finetuned_models/roberta-base/example \
    --per_device_train_batch_size=8 \
    --gradient_accumulation_steps=4 \
    --learning_rate=2.5e-4 \
    --num_train_epochs=3 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16


!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-base/example \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words_eval.txt \
  --k_words=5 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="sample_output.txt" \
  --fp16

!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="roberta-large" \
    --train_file=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words.txt \
    --do_train \
    --output_dir=finetuned_models/roberta-base/example \
    --per_device_train_batch_size=8 \
    --gradient_accumulation_steps=4 \
    --learning_rate=2.5e-4 \
    --num_train_epochs=5 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16


!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-base/example \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words_eval.txt \
  --k_words=5 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="sample_output.txt" \
  --fp16

!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="roberta-large" \
    --train_file=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words.txt \
    --do_train \
    --output_dir=finetuned_models/roberta-base/example \
    --per_device_train_batch_size=8 \
    --gradient_accumulation_steps=4 \
    --learning_rate=2.5e-4 \
    --num_train_epochs=10 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16


!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-base/example \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words_eval.txt \
  --k_words=5 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="sample_output.txt" \
  --fp16

!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="roberta-large" \
    --train_file=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words.txt \
    --do_train \
    --output_dir=finetuned_models/roberta-base/example \
    --per_device_train_batch_size=8 \
    --gradient_accumulation_steps=4 \
    --learning_rate=2.5e-4 \
    --num_train_epochs=15 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16


!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-base/example \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words_eval.txt \
  --k_words=5 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="sample_output.txt" \
  --fp16

!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="roberta-large" \
    --train_file=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words.txt \
    --do_train \
    --output_dir=finetuned_models/roberta-base/example \
    --per_device_train_batch_size=8 \
    --gradient_accumulation_steps=4 \
    --learning_rate=2.5e-4 \
    --num_train_epochs=20 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16


!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-base/example \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words_eval.txt \
  --k_words=5 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="sample_output.txt" \
  --fp16

!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="roberta-large" \
    --train_file=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words.txt \
    --do_train \
    --output_dir=finetuned_models/roberta-base/example \
    --per_device_train_batch_size=8 \
    --gradient_accumulation_steps=4 \
    --learning_rate=2.5e-4 \
    --num_train_epochs=30 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16


!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-base/example \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words_eval.txt \
  --k_words=5 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="sample_output.txt" \
  --fp16

!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="roberta-large" \
    --train_file=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words.txt \
    --do_train \
    --output_dir=finetuned_models/roberta-base/example \
    --per_device_train_batch_size=8 \
    --gradient_accumulation_steps=4 \
    --learning_rate=2.5e-4 \
    --num_train_epochs=50 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16


!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-base/example \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words_eval.txt \
  --k_words=5 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="sample_output.txt" \
  --fp16

!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path="roberta-large" \
    --train_file=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words.txt \
    --do_train \
    --output_dir=finetuned_models/roberta-base/example \
    --per_device_train_batch_size=8 \
    --gradient_accumulation_steps=4 \
    --learning_rate=2.5e-4 \
    --num_train_epochs=100 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \
    --fp16


!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-base/example \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/example100words_eval.txt \
  --k_words=5 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="sample_output.txt" \
  --fp16

"""# **Fine-Tuning definitivo a RoBERTa Large con el corpus entero de WordNet**"""

!python3 /content/drive/MyDrive/TFG/transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-large/definitivo/checkpoint-18000 \
    --train_file=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/wordnet_train.txt \
    --do_train \
    --output_dir=finetuned_models/roberta-large/definitivo \
    --per_device_train_batch_size=8 \
    --gradient_accumulation_steps=4 \
    --learning_rate=1e-4 \
    --num_train_epochs=10 \
    --save_steps=6000 \
    --overwrite_output_dir \
    --overwrite_cache \
    --line_by_line \

"""# **Evaluando el modelo base y el fine-tuneado con el corpus entero de WordNet y 1000 definiciones de otros diccionarios**"""

!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path="roberta-large" \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/wordnet_eval.txt \
  --k_words=5 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="sample_output_base.txt" \
  --fp16

!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-large/definitivo \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/wordnet_eval.txt \
  --k_words=5 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="sample_output_finetuned.txt" \
  --fp16

!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path="roberta-large" \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/def_1000_eval.txt \
  --k_words=5 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="ldoce_output_base.txt" \
  --fp16

!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-large/definitivo \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/def_1000_eval.txt \
  --k_words=5 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="ldoce_output_finetuned.txt" \
  --fp16

"""# **En estos 3 bloques de código, se evalúa el modelo final con el conjunto de datos comparable**"""

!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-large/definitivo \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/seen_eval.txt \
  --k_words=100 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="seen_results.txt" \
  --fp16

!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-large/definitivo \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/unseen_eval.txt \
  --k_words=100 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="unseen_results.txt" \
  --fp16

!python3 /content/drive/MyDrive/TFG/Finetunning/eval.py \
  --model_name_or_path=/content/drive/MyDrive/TFG/Finetunning/finetuned_models/roberta-large/definitivo \
  --dataset_path=/content/drive/MyDrive/TFG/Finetunning/cleaned_data/desc_eval.txt \
  --k_words=100 \
  --batch_size=32 \
  --do_eval \
  --do_predict \
  --output_path="desc_results.txt" \
  --fp16