# -*- coding: utf-8 -*-
"""Sentence-transformers

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uCrzXAZRxn3HjkJvpiV9r2aKwn3w79sF
"""

!pip install transformers
!pip install torch
!pip install tqdm
!pip install scikit-learn
!pip install numpy
!pip install scipy
!pip install nltk
!pip install sentencepiece
!pip install fastNLP
!pip install fitlog

!pip install -U sentence-transformers
!pip install faiss-cpu --no-cache

from google.colab import drive
drive.mount("/content/drive")

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-MiniLM-L6-v2', device='cuda')

import json

folder = "/content/drive/MyDrive/TFG/Sentence-transformers/data/en/"
files = ["train.json", "dev.json"]

sentences = []
words = []

for file in files:
  f = open(folder+file, "r")

  cont = 0

  data = json.load(f)
  for i in data:    
    words.append(i['word'])
    sentences.append(i['definitions'])
    cont += 1

sentence_embeddings = model.encode(sentences)
final_embeddings= zip(words, sentence_embeddings)

import faiss
import statistics

folder = "/content/drive/MyDrive/TFG/Sentence-transformers/data/en/"
files = ["seen.json", "unseen.json", "desc.json"]
for file in files:
  f = open(folder+file, "r")
  output_file = open(file+"_Sentence-Transformers_predictions.txt", "w")

  data = json.load(f)
  queries = []
  key_words = []
  cont2 = 0
  for i in data:
    key_words.append(i['word'])
    queries.append(i['definitions'])
    cont2 += 1


  precision_1 = 0
  precision_10 = 0
  precision_100 = 0
  median_and_variance = []

  query_embeddings = model.encode(queries)

  d = len(sentence_embeddings[0])
  nb = cont
  nq = len(queries)

  xb = sentence_embeddings
  xq = query_embeddings

  index = faiss.IndexFlatL2(d)
  index.add(xb)

  k = 100
  D, I = index.search(xq, k)
  for i, query in enumerate(queries):
    output_file.write(key_words[i] + " ||| ")
    for prediction in range(k):
      output_file.write(words[I[i][prediction]] + ", ")
      if str.lower(key_words[i]) == str.lower(words[I[i][prediction]]):
        median_and_variance.append(prediction)
        if prediction <= 0:
          precision_1 += 1
        if prediction <= 9:
          precision_10 += 1
        if prediction <= 99:
          precision_100 += 1
        break
    output_file.write("\n") 

    
  prec1 = precision_1 / nq
  prec10 = precision_10 / nq
  prec100 = precision_100 / nq
  mediana = statistics.median(median_and_variance)
  varianza = statistics.variance(median_and_variance)

  print(file)
  print("Mediana: " + str(mediana))
  print("Precision P@1: {:.2%}".format(prec1))
  print("Precision P@10: {:.2%}".format(prec10))
  print("Precision P@100: {:.2%}".format(prec100))
  print("Varianza: " + str(varianza)) 
  print()
  print()

  f.close()
  output_file.close()